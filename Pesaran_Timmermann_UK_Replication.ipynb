{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c84db44",
   "metadata": {},
   "source": [
    "# Replicating Pesaran & Timmermann (1994) for UK Equity Markets\n",
    "## Complete Implementation Guide for Forecasting FTSE All-Share Excess Returns\n",
    "\n",
    "**Author:** Manpreet Sangha [210048348]  \n",
    "**Course:** SMM265 Asset Pricing  \n",
    "**Date:** December 2025\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "This notebook replicates the methodology from:\n",
    "> Pesaran, M. H. and Timmermann, A. (1994) \"Forecasting Stock Returns: An Examination of Stock Market Trading in the Presence of Transaction Costs\", Journal of Forecasting, Vol. 13, pp. 335-367.\n",
    "\n",
    "**Adaptation for UK Markets:**\n",
    "- **Market:** FTSE All-Share Index\n",
    "- **Forecast Horizon:** 6 months (semi-annual)  \n",
    "- **Evaluation Period:** October 2015 ‚Äì October 2025\n",
    "- **Training Data:** 1990 ‚Äì October 2015 (expanding window)\n",
    "\n",
    "**Key Objectives:**\n",
    "1. Predict excess stock returns using macroeconomic variables\n",
    "2. Implement recursive out-of-sample forecasting\n",
    "3. Evaluate directional accuracy using Pesaran-Timmermann sign test\n",
    "4. Test switching strategy performance vs buy-and-hold\n",
    "5. Analyze robustness under transaction costs\n",
    "\n",
    "**Expected Deliverables:**\n",
    "- Regression model with UK predictors (DY, PI12, DI12, DIP12, TERM)\n",
    "- 20 recursive out-of-sample predictions (6-month periods)\n",
    "- Statistical significance tests of forecasting ability\n",
    "- Portfolio performance analysis with transaction cost scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c26be",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for the analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Statistical and econometric libraries\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, acorr_breusch_godfrey\n",
    "from statsmodels.stats.stattools import jarque_bera, durbin_watson\n",
    "from statsmodels.stats.outliers_influence import reset_ramsey\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting parameters\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define global constants\n",
    "BASE_PATH = Path(r\"C:\\Users\\Manpreet\\OneDrive - City St George's, University of London\\Documents\\Term1\\Coursework\\SMM265 Asset Pricing\\Q1\")\n",
    "RAW_DATA_PATH = BASE_PATH / \"Data\" / \"Raw Data\"\n",
    "CLEANED_DATA_PATH = BASE_PATH / \"Data\" / \"Cleaned Data\"\n",
    "OUTPUT_PATH = BASE_PATH / \"Python Scripts\" / \"Asset-Pricing\" / \"outputs\"\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "for path in [OUTPUT_PATH, OUTPUT_PATH / \"tables\", OUTPUT_PATH / \"figures\"]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Key dates for analysis\n",
    "TRAINING_START = '1990-01-31'  # Start of training period\n",
    "FORECAST_START = '2015-10-31'  # Start of out-of-sample period\n",
    "FORECAST_END = '2025-10-31'    # End of evaluation period\n",
    "FORECAST_HORIZON = 6           # 6-month forecast horizon\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Analysis period: {TRAINING_START} to {FORECAST_END}\")\n",
    "print(f\"Out-of-sample evaluation: {FORECAST_START} to {FORECAST_END}\")\n",
    "print(f\"Forecast horizon: {FORECAST_HORIZON} months\")\n",
    "print(f\"Output directory: {OUTPUT_PATH}\")\n",
    "\n",
    "# Display key thresholds from the paper\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPECTED RESULTS BASED ON PESARAN & TIMMERMANN (1994)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Expected R¬≤: 0.15 - 0.30 (between quarterly and annual)\")\n",
    "print(\"Expected Sign Accuracy: 55% - 70%\")\n",
    "print(\"Expected PT Statistic: 1.5 - 2.5\")\n",
    "print(\"Critical Values - PT Test: 1.645 (5%), 2.326 (1%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82877b8c",
   "metadata": {},
   "source": [
    "## 2. Load and Process Financial Data\n",
    "\n",
    "This section loads all required financial and economic data:\n",
    "- **FTSE All-Share Total Return Index** (monthly, 1990-2025)\n",
    "- **FTSE All-Share Dividend Yield** (monthly, 1990-2025)\n",
    "- **UK 3-Month Treasury Bill Rate** (monthly, 1989-2025)\n",
    "- **UK Consumer Price Index (CPI)** (monthly, 1989-2025)\n",
    "- **UK Industrial Production Index** (monthly, 1989-2025)\n",
    "- **UK 10-Year Gilt Yield** (optional, monthly, 1990-2025)\n",
    "\n",
    "**Data Sources:**\n",
    "- Bank of England Statistical Database\n",
    "- Office for National Statistics (ONS)\n",
    "- Yahoo Finance / Bloomberg for equity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_data(file_pattern, data_type=\"financial\", monthly_path=\"Monthly\", \n",
    "                   skip_rows=5, date_col=0, value_col=1, \n",
    "                   date_name=\"Date\", value_name=\"Value\"):\n",
    "    \"\"\"\n",
    "    Generic function to load and clean Excel data files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_pattern : str\n",
    "        Pattern to match Excel files\n",
    "    data_type : str\n",
    "        Description for logging\n",
    "    monthly_path : str\n",
    "        Subfolder containing monthly data\n",
    "    skip_rows : int\n",
    "        Number of rows to skip from top\n",
    "    date_col : int\n",
    "        Column index for dates\n",
    "    value_col : int\n",
    "        Column index for values\n",
    "    date_name : str\n",
    "        Name for date column\n",
    "    value_name : str\n",
    "        Name for value column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Look for files matching the pattern\n",
    "        data_dir = CLEANED_DATA_PATH / monthly_path\n",
    "        files = list(data_dir.glob(file_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è  No files found matching pattern: {file_pattern}\")\n",
    "            print(f\"   Looking in: {data_dir}\")\n",
    "            return None\n",
    "            \n",
    "        file_path = files[0]  # Use first match\n",
    "        print(f\"üìÅ Loading {data_type}: {file_path.name}\")\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Skip metadata rows\n",
    "        if skip_rows > 0:\n",
    "            df = df.iloc[skip_rows:].copy()\n",
    "        \n",
    "        # Clean empty rows/columns\n",
    "        df = df.dropna(how='all').dropna(axis=1, how='all').reset_index(drop=True)\n",
    "        \n",
    "        # Select and rename columns\n",
    "        if len(df.columns) >= max(date_col + 1, value_col + 1):\n",
    "            df = df.iloc[:, [date_col, value_col]].copy()\n",
    "            df.columns = [date_name, value_name]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Expected at least {max(date_col + 1, value_col + 1)} columns, found {len(df.columns)}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert date column\n",
    "        df[date_name] = pd.to_datetime(df[date_name], errors='coerce')\n",
    "        \n",
    "        # Convert value column to numeric\n",
    "        df[value_name] = pd.to_numeric(df[value_name], errors='coerce')\n",
    "        \n",
    "        # Remove rows with invalid dates or values\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Sort by date\n",
    "        df = df.sort_values(date_name).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded {len(df)} observations from {df[date_name].min().strftime('%Y-%m')} to {df[date_name].max().strftime('%Y-%m')}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {data_type}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"\n",
    "    Create sample data for demonstration if real data files are not available.\n",
    "    This generates realistic synthetic data based on historical UK patterns.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create date range from 1989 to 2025 (monthly)\n",
    "    dates = pd.date_range(start='1989-01-31', end='2025-10-31', freq='M')\n",
    "    n_obs = len(dates)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create synthetic data based on realistic UK historical patterns\n",
    "    \n",
    "    # FTSE All-Share (starts at 1000, trending upward with volatility)\n",
    "    ftse_growth = 0.005  # 0.5% average monthly growth\n",
    "    ftse_vol = 0.04      # 4% monthly volatility\n",
    "    ftse_returns = np.random.normal(ftse_growth, ftse_vol, n_obs)\n",
    "    ftse_prices = [1000]  # Starting value\n",
    "    for ret in ftse_returns:\n",
    "        ftse_prices.append(ftse_prices[-1] * (1 + ret))\n",
    "    ftse_prices = ftse_prices[1:]  # Remove initial value\n",
    "    \n",
    "    # Dividend Yield (starts around 4%, mean-reverting)\n",
    "    div_yield_mean = 3.5\n",
    "    div_yield_vol = 0.5\n",
    "    div_yield = div_yield_mean + np.random.normal(0, div_yield_vol, n_obs).cumsum() * 0.1\n",
    "    div_yield = np.clip(div_yield, 1.0, 7.0)  # Keep between 1% and 7%\n",
    "    \n",
    "    # UK 3-Month T-Bill Rate (starts around 10%, trending down)\n",
    "    tbill_trend = np.linspace(10, 1, n_obs)  # Declining trend from 10% to 1%\n",
    "    tbill_vol = 0.3\n",
    "    tbill_rate = tbill_trend + np.random.normal(0, tbill_vol, n_obs).cumsum() * 0.1\n",
    "    tbill_rate = np.clip(tbill_rate, 0.1, 15.0)  # Keep positive and reasonable\n",
    "    \n",
    "    # UK CPI (starting at 100, inflation trend)\n",
    "    cpi_growth = 0.002  # 0.2% average monthly inflation\n",
    "    cpi_vol = 0.003     # 0.3% monthly volatility\n",
    "    cpi_returns = np.random.normal(cpi_growth, cpi_vol, n_obs)\n",
    "    cpi_values = [100]  # Starting at 100\n",
    "    for ret in cpi_returns:\n",
    "        cpi_values.append(cpi_values[-1] * (1 + ret))\n",
    "    cpi_values = cpi_values[1:]\n",
    "    \n",
    "    # UK Industrial Production (starting at 100, cyclical)\n",
    "    ip_trend = 0.001    # 0.1% average monthly growth\n",
    "    ip_vol = 0.01       # 1% monthly volatility\n",
    "    ip_cycle = 0.5 * np.sin(2 * np.pi * np.arange(n_obs) / 60)  # 5-year cycle\n",
    "    ip_returns = np.random.normal(ip_trend, ip_vol, n_obs) + ip_cycle * 0.01\n",
    "    ip_values = [100]   # Starting at 100\n",
    "    for ret in ip_returns:\n",
    "        ip_values.append(ip_values[-1] * (1 + ret))\n",
    "    ip_values = ip_values[1:]\n",
    "    \n",
    "    # UK 10-Year Gilt (related to T-bill with spread)\n",
    "    gilt_spread = 1.5 + np.random.normal(0, 0.5, n_obs).cumsum() * 0.1\n",
    "    gilt_spread = np.clip(gilt_spread, 0, 4)  # Keep spread positive and reasonable\n",
    "    gilt_yield = tbill_rate + gilt_spread\n",
    "    \n",
    "    # Create DataFrames\n",
    "    datasets = {\n",
    "        'ftse': pd.DataFrame({'Date': dates, 'ftse_price': ftse_prices}),\n",
    "        'dividend': pd.DataFrame({'Date': dates, 'dividend_yield': div_yield}),\n",
    "        'tbill': pd.DataFrame({'Date': dates, 'uktb_yield': tbill_rate}),\n",
    "        'cpi': pd.DataFrame({'Date': dates, 'cpi_index': cpi_values}),\n",
    "        'ip': pd.DataFrame({'Date': dates, 'ip_index': ip_values}),\n",
    "        'gilt': pd.DataFrame({'Date': dates, 'gilt_yield': gilt_yield})\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Created sample data with {n_obs} monthly observations\")\n",
    "    print(f\"   üìÖ Period: {dates[0].strftime('%Y-%m')} to {dates[-1].strftime('%Y-%m')}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Load all required datasets\n",
    "print(\"üöÄ LOADING FINANCIAL AND ECONOMIC DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Try to load real data first, fall back to sample data if unavailable\n",
    "data = {}\n",
    "\n",
    "# Load each dataset\n",
    "file_patterns = {\n",
    "    'ftse': '*FTSE*All*Share*.xlsx',\n",
    "    'dividend': '*dividend*yield*.xlsx',\n",
    "    'tbill': '*Treasury*Bill*.xlsx',\n",
    "    'cpi': '*CPI*.xlsx', \n",
    "    'ip': '*Industrial*Production*.xlsx',\n",
    "    'gilt': '*Gilt*.xlsx'\n",
    "}\n",
    "\n",
    "value_names = {\n",
    "    'ftse': 'ftse_price',\n",
    "    'dividend': 'dividend_yield', \n",
    "    'tbill': 'uktb_yield',\n",
    "    'cpi': 'cpi_index',\n",
    "    'ip': 'ip_index',\n",
    "    'gilt': 'gilt_yield'\n",
    "}\n",
    "\n",
    "# Try loading real data\n",
    "real_data_loaded = 0\n",
    "for key, pattern in file_patterns.items():\n",
    "    df = load_excel_data(pattern, data_type=key.upper(), value_name=value_names[key])\n",
    "    if df is not None:\n",
    "        data[key] = df\n",
    "        real_data_loaded += 1\n",
    "\n",
    "# If most real data is missing, use sample data\n",
    "if real_data_loaded < 3:\n",
    "    print(\"\\n‚ö†Ô∏è  Limited real data available. Using sample data for demonstration.\")\n",
    "    print(\"   For actual coursework, replace with real data files.\\n\")\n",
    "    data = create_sample_data()\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Successfully loaded {real_data_loaded} real datasets\")\n",
    "\n",
    "print(f\"\\nüìä Available datasets: {list(data.keys())}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2315d07",
   "metadata": {},
   "source": [
    "## 3. Construct Predictor Variables\n",
    "\n",
    "Following Pesaran & Timmermann (1994), we construct five key predictor variables:\n",
    "\n",
    "| Variable | Formula | Lag | Expected Sign | Economic Rationale |\n",
    "|----------|---------|-----|---------------|-------------------|\n",
    "| **DY** | Dividend yield (level) | t‚àí1 | **+** | High yield = undervalued = higher future returns |\n",
    "| **PI12** | log(CPI_t / CPI_{t-12}) | t‚àí2 | **‚àí** | High inflation = uncertainty = lower returns |\n",
    "| **DI12** | I3m_t ‚àí I3m_{t-12} | t‚àí1 | **‚àí** | Rising rates = tighter money = lower returns |\n",
    "| **DIP12** | log(IP_t / IP_{t-12}) | t‚àí2 | **‚àí** | Strong economy = low risk premium = lower expected returns |\n",
    "| **TERM** | I10y_t ‚àí I3m_t | t‚àí1 | **+** | Wide spread = recession expected = higher risk premium |\n",
    "\n",
    "**Key Points:**\n",
    "- All variables use appropriate lags to avoid look-ahead bias\n",
    "- 2-month lag for macro data (CPI, IP) reflects publication delays\n",
    "- 1-month lag for financial data (rates, yields) reflects immediate availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397527d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_predictor_variables(data_dict):\n",
    "    \"\"\"\n",
    "    Construct all predictor variables following Pesaran & Timmermann (1994).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Dictionary containing loaded datasets\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataset with all predictor variables\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîß CONSTRUCTING PREDICTOR VARIABLES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Start with the most complete dataset (usually FTSE)\n",
    "    base_df = data_dict['ftse'].copy()\n",
    "    base_df = base_df.set_index('Date')\n",
    "    \n",
    "    print(f\"üìÖ Base period: {base_df.index.min().strftime('%Y-%m')} to {base_df.index.max().strftime('%Y-%m')}\")\n",
    "    \n",
    "    # Merge all datasets on date\n",
    "    for key, df in data_dict.items():\n",
    "        if key != 'ftse':\n",
    "            df_merge = df.set_index('Date')\n",
    "            base_df = base_df.join(df_merge, how='outer')\n",
    "    \n",
    "    print(f\"üìä Combined dataset shape: {base_df.shape}\")\n",
    "    print(f\"üìä Available columns: {list(base_df.columns)}\")\n",
    "    \n",
    "    # Forward fill missing values (up to 2 periods for monthly data)\n",
    "    base_df = base_df.fillna(method='ffill', limit=2)\n",
    "    \n",
    "    # Sort by date to ensure proper time series order\n",
    "    base_df = base_df.sort_index()\n",
    "    \n",
    "    # Construct predictor variables\n",
    "    print(\"\\nüèóÔ∏è  Constructing individual predictors...\")\n",
    "    \n",
    "    # 1. Dividend Yield (DY) - level, lagged 1 month\n",
    "    if 'dividend_yield' in base_df.columns:\n",
    "        base_df['DY'] = base_df['dividend_yield'].shift(1) / 100  # Convert to decimal and lag\n",
    "        print(\"   ‚úÖ DY: Dividend yield (t-1)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Dividend yield data not available - using proxy\")\n",
    "        # Use a proxy based on FTSE level (inverse relationship)\n",
    "        ftse_ma = base_df['ftse_price'].rolling(12).mean()\n",
    "        base_df['DY'] = (4.0 * ftse_ma.iloc[0] / ftse_ma).shift(1) / 100  # Starts at 4%\n",
    "    \n",
    "    # 2. 12-month Inflation (PI12) - log difference, lagged 2 months\n",
    "    if 'cpi_index' in base_df.columns:\n",
    "        base_df['PI12'] = (np.log(base_df['cpi_index']) - np.log(base_df['cpi_index'].shift(12))).shift(2)\n",
    "        print(\"   ‚úÖ PI12: 12-month inflation (t-2)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  CPI data not available - using synthetic inflation\")\n",
    "        base_df['PI12'] = (np.random.normal(0.02, 0.01, len(base_df))).shift(2)\n",
    "    \n",
    "    # 3. 12-month Change in Interest Rate (DI12) - difference, lagged 1 month\n",
    "    if 'uktb_yield' in base_df.columns:\n",
    "        base_df['DI12'] = (base_df['uktb_yield'] - base_df['uktb_yield'].shift(12)).shift(1)\n",
    "        print(\"   ‚úÖ DI12: 12-month rate change (t-1)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  T-bill data not available - using synthetic rates\")\n",
    "        synthetic_rate = 5 + np.random.normal(0, 2, len(base_df)).cumsum() * 0.1\n",
    "        base_df['uktb_yield'] = synthetic_rate\n",
    "        base_df['DI12'] = (synthetic_rate - pd.Series(synthetic_rate).shift(12)).shift(1)\n",
    "    \n",
    "    # 4. 12-month Change in Industrial Production (DIP12) - log difference, lagged 2 months  \n",
    "    if 'ip_index' in base_df.columns:\n",
    "        base_df['DIP12'] = (np.log(base_df['ip_index']) - np.log(base_df['ip_index'].shift(12))).shift(2)\n",
    "        print(\"   ‚úÖ DIP12: 12-month IP change (t-2)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  IP data not available - using synthetic IP\")\n",
    "        base_df['DIP12'] = (np.random.normal(0.01, 0.02, len(base_df))).shift(2)\n",
    "    \n",
    "    # 5. Term Spread (TERM) - long-short rate difference, lagged 1 month\n",
    "    if 'gilt_yield' in base_df.columns and 'uktb_yield' in base_df.columns:\n",
    "        base_df['TERM'] = (base_df['gilt_yield'] - base_df['uktb_yield']).shift(1)\n",
    "        print(\"   ‚úÖ TERM: Term spread (t-1)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Gilt data not available - creating synthetic term spread\")\n",
    "        if 'uktb_yield' in base_df.columns:\n",
    "            base_df['TERM'] = (base_df['uktb_yield'] + 1.5 + np.random.normal(0, 0.5, len(base_df))).shift(1)\n",
    "        else:\n",
    "            base_df['TERM'] = np.random.normal(1.5, 0.5, len(base_df))\n",
    "    \n",
    "    print(f\"\\nüìã Summary of constructed variables:\")\n",
    "    predictor_cols = ['DY', 'PI12', 'DI12', 'DIP12', 'TERM']\n",
    "    for col in predictor_cols:\n",
    "        if col in base_df.columns:\n",
    "            valid_obs = base_df[col].notna().sum()\n",
    "            mean_val = base_df[col].mean()\n",
    "            std_val = base_df[col].std()\n",
    "            print(f\"   {col:6}: {valid_obs:4d} obs, mean={mean_val:7.4f}, std={std_val:7.4f}\")\n",
    "    \n",
    "    return base_df\n",
    "\n",
    "\n",
    "def create_excess_returns(df, horizon=6):\n",
    "    \"\"\"\n",
    "    Calculate 6-month excess returns for FTSE All-Share.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with FTSE prices and T-bill rates\n",
    "    horizon : int\n",
    "        Forecast horizon in months\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with excess returns added\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìà CALCULATING {horizon}-MONTH EXCESS RETURNS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate FTSE total returns (assuming prices include dividends)\n",
    "    # 6-month nominal return: (Price_t / Price_{t-6}) - 1\n",
    "    df['ftse_return_6m'] = (df['ftse_price'] / df['ftse_price'].shift(horizon)) - 1\n",
    "    \n",
    "    # Calculate 6-month risk-free return\n",
    "    # Convert annual T-bill rate to 6-month rate\n",
    "    df['rf_6m'] = ((1 + df['uktb_yield'] / 100) ** (horizon/12)) - 1\n",
    "    \n",
    "    # Shift risk-free rate to align with return period\n",
    "    # Use rate known at beginning of 6-month period\n",
    "    df['rf_6m_lagged'] = df['rf_6m'].shift(horizon)\n",
    "    \n",
    "    # Calculate excess returns\n",
    "    df['excess_return_6m'] = df['ftse_return_6m'] - df['rf_6m_lagged']\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    valid_excess = df['excess_return_6m'].dropna()\n",
    "    \n",
    "    print(f\"üìä Excess return statistics:\")\n",
    "    print(f\"   Observations: {len(valid_excess)}\")\n",
    "    print(f\"   Mean: {valid_excess.mean():.4f} ({valid_excess.mean()*100:.2f}%)\")\n",
    "    print(f\"   Std Dev: {valid_excess.std():.4f} ({valid_excess.std()*100:.2f}%)\")\n",
    "    print(f\"   Min: {valid_excess.min():.4f} ({valid_excess.min()*100:.2f}%)\")\n",
    "    print(f\"   Max: {valid_excess.max():.4f} ({valid_excess.max()*100:.2f}%)\")\n",
    "    print(f\"   Positive periods: {(valid_excess > 0).sum()} ({(valid_excess > 0).mean()*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Execute variable construction\n",
    "model_data = construct_predictor_variables(data)\n",
    "model_data = create_excess_returns(model_data, horizon=FORECAST_HORIZON)\n",
    "\n",
    "# Display final dataset info\n",
    "print(f\"\\nüìã FINAL MODEL DATASET\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {model_data.shape}\")\n",
    "print(f\"Period: {model_data.index.min().strftime('%Y-%m')} to {model_data.index.max().strftime('%Y-%m')}\")\n",
    "print(f\"Columns: {list(model_data.columns)}\")\n",
    "\n",
    "# Check for missing values in key variables\n",
    "key_vars = ['excess_return_6m', 'DY', 'PI12', 'DI12', 'DIP12', 'TERM']\n",
    "missing_summary = model_data[key_vars].isnull().sum()\n",
    "print(f\"\\nMissing values by variable:\")\n",
    "for var in key_vars:\n",
    "    if var in model_data.columns:\n",
    "        missing = missing_summary[var] if var in missing_summary else 0\n",
    "        print(f\"   {var:15}: {missing:4d} missing\")\n",
    "\n",
    "print(\"\\n‚úÖ Variable construction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e5ffa",
   "metadata": {},
   "source": [
    "## 4. Calculate Excess Returns\n",
    "\n",
    "The dependent variable is the **6-month excess return** on FTSE All-Share:\n",
    "\n",
    "**ERFTSE_t = NRFTSE_t ‚àí RF_{t-6}**\n",
    "\n",
    "Where:\n",
    "- **NRFTSE_t** = 6-month nominal return on FTSE All-Share (including dividends)\n",
    "- **RF_{t-6}** = 6-month risk-free rate known at start of period\n",
    "\n",
    "**Key Implementation Details:**\n",
    "- Use Total Return Index when available (includes dividend reinvestment)\n",
    "- Convert annual T-bill rate to 6-month equivalent: `(1 + annual_rate)^0.5 - 1`\n",
    "- Align timing: risk-free rate known at beginning of forecast period\n",
    "- Handle missing data through forward-filling and interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435794bc",
   "metadata": {},
   "source": [
    "## 5. Full Sample Regression Analysis\n",
    "\n",
    "Estimate the baseline regression model using all available data:\n",
    "\n",
    "**Primary Model (4 predictors):**\n",
    "```\n",
    "ERFTSE_t = Œ± + Œ≤‚ÇÅ√óDY_{t-1} + Œ≤‚ÇÇ√óPI12_{t-2} + Œ≤‚ÇÉ√óDI12_{t-1} + Œ≤‚ÇÑ√óDIP12_{t-2} + Œµ_t\n",
    "```\n",
    "\n",
    "**Extended Model (5 predictors):**\n",
    "```\n",
    "ERFTSE_t = Œ± + Œ≤‚ÇÅ√óDY_{t-1} + Œ≤‚ÇÇ√óPI12_{t-2} + Œ≤‚ÇÉ√óDI12_{t-1} + Œ≤‚ÇÑ√óDIP12_{t-2} + Œ≤‚ÇÖ√óTERM_{t-1} + Œµ_t\n",
    "```\n",
    "\n",
    "**Expected Coefficient Signs:**\n",
    "- DY (Dividend Yield): **Positive** (+) - High yield indicates undervaluation\n",
    "- PI12 (Inflation): **Negative** (‚àí) - High inflation reduces real returns  \n",
    "- DI12 (Rate Change): **Negative** (‚àí) - Rising rates hurt equity valuations\n",
    "- DIP12 (IP Change): **Negative** (‚àí) - Strong economy reduces risk premium\n",
    "- TERM (Term Spread): **Positive** (+) - Wide spread indicates recession risk\n",
    "\n",
    "**Diagnostic Tests:**\n",
    "- Durbin-Watson (serial correlation)\n",
    "- Jarque-Bera (normality)\n",
    "- Breusch-Pagan (heteroscedasticity)\n",
    "- RESET (functional form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_sample_regression(df, predictors=['DY', 'PI12', 'DI12', 'DIP12'], \n",
    "                              target='excess_return_6m', include_term=False):\n",
    "    \"\"\"\n",
    "    Run full sample OLS regression with diagnostic tests.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Model dataset\n",
    "    predictors : list\n",
    "        List of predictor variable names\n",
    "    target : str\n",
    "        Target variable name\n",
    "    include_term : bool\n",
    "        Whether to include term spread\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Regression results and diagnostics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìä FULL SAMPLE REGRESSION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare data\n",
    "    if include_term and 'TERM' not in predictors:\n",
    "        predictors = predictors + ['TERM']\n",
    "    \n",
    "    # Create regression dataset (drop missing values)\n",
    "    reg_vars = [target] + predictors\n",
    "    reg_data = df[reg_vars].dropna()\n",
    "    \n",
    "    print(f\"üìã Regression specification:\")\n",
    "    print(f\"   Dependent variable: {target}\")\n",
    "    print(f\"   Predictors: {', '.join(predictors)}\")\n",
    "    print(f\"   Sample size: {len(reg_data)} observations\")\n",
    "    print(f\"   Sample period: {reg_data.index.min().strftime('%Y-%m')} to {reg_data.index.max().strftime('%Y-%m')}\")\n",
    "    \n",
    "    # Prepare variables\n",
    "    y = reg_data[target]\n",
    "    X = reg_data[predictors]\n",
    "    X_with_const = sm.add_constant(X)  # Add intercept\n",
    "    \n",
    "    # Estimate OLS regression\n",
    "    model = sm.OLS(y, X_with_const).fit()\n",
    "    \n",
    "    print(f\"\\nüìà REGRESSION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Extract key statistics\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'n_obs': len(reg_data),\n",
    "        'r_squared': model.rsquared,\n",
    "        'adj_r_squared': model.rsquared_adj,\n",
    "        'f_statistic': model.fvalue,\n",
    "        'f_pvalue': model.f_pvalue,\n",
    "        'coefficients': dict(zip(['const'] + predictors, model.params)),\n",
    "        'std_errors': dict(zip(['const'] + predictors, model.bse)),\n",
    "        't_stats': dict(zip(['const'] + predictors, model.tvalues)),\n",
    "        'p_values': dict(zip(['const'] + predictors, model.pvalues)),\n",
    "        'residuals': model.resid,\n",
    "        'fitted_values': model.fittedvalues\n",
    "    }\n",
    "    \n",
    "    # Diagnostic tests\n",
    "    print(f\"\\nüîç DIAGNOSTIC TESTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Durbin-Watson test (serial correlation)\n",
    "    dw_stat = durbin_watson(results['residuals'])\n",
    "    print(f\"Durbin-Watson statistic: {dw_stat:.3f}\")\n",
    "    if dw_stat < 1.5:\n",
    "        print(\"   ‚Üí Positive serial correlation detected\")\n",
    "    elif dw_stat > 2.5:\n",
    "        print(\"   ‚Üí Negative serial correlation detected\") \n",
    "    else:\n",
    "        print(\"   ‚Üí No strong evidence of serial correlation\")\n",
    "    \n",
    "    # 2. Jarque-Bera test (normality)\n",
    "    jb_stat, jb_pvalue = jarque_bera(results['residuals'])\n",
    "    print(f\"Jarque-Bera test: {jb_stat:.3f} (p-value: {jb_pvalue:.4f})\")\n",
    "    if jb_pvalue < 0.05:\n",
    "        print(\"   ‚Üí Residuals are not normally distributed (expected for returns)\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Cannot reject normality\")\n",
    "    \n",
    "    # 3. Breusch-Pagan test (heteroscedasticity)\n",
    "    bp_stat, bp_pvalue, _, _ = het_breuschpagan(results['residuals'], X_with_const)\n",
    "    print(f\"Breusch-Pagan test: {bp_stat:.3f} (p-value: {bp_pvalue:.4f})\")\n",
    "    if bp_pvalue < 0.05:\n",
    "        print(\"   ‚Üí Heteroscedasticity detected\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Homoscedasticity cannot be rejected\")\n",
    "    \n",
    "    # 4. RESET test (functional form)\n",
    "    try:\n",
    "        reset_stat, reset_pvalue, _ = reset_ramsey(results['residuals'], X_with_const)\n",
    "        print(f\"RESET test: {reset_stat:.3f} (p-value: {reset_pvalue:.4f})\")\n",
    "        if reset_pvalue < 0.05:\n",
    "            print(\"   ‚Üí Model specification may be inadequate\")\n",
    "        else:\n",
    "            print(\"   ‚Üí No evidence of specification error\")\n",
    "    except:\n",
    "        print(\"RESET test: Could not compute (insufficient observations)\")\n",
    "        reset_stat, reset_pvalue = np.nan, np.nan\n",
    "    \n",
    "    # Store diagnostic results\n",
    "    results.update({\n",
    "        'durbin_watson': dw_stat,\n",
    "        'jarque_bera': (jb_stat, jb_pvalue),\n",
    "        'breusch_pagan': (bp_stat, bp_pvalue),\n",
    "        'reset': (reset_stat, reset_pvalue)\n",
    "    })\n",
    "    \n",
    "    # Coefficient analysis\n",
    "    print(f\"\\nüìã COEFFICIENT ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    expected_signs = {'DY': '+', 'PI12': '-', 'DI12': '-', 'DIP12': '-', 'TERM': '+'}\n",
    "    \n",
    "    print(f\"{'Variable':<8} {'Coeff':<10} {'Std Err':<10} {'t-stat':<8} {'p-value':<8} {'Expected':<8} {'Sign OK?':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for var in ['const'] + predictors:\n",
    "        if var in results['coefficients']:\n",
    "            coef = results['coefficients'][var]\n",
    "            se = results['std_errors'][var] \n",
    "            t_stat = results['t_stats'][var]\n",
    "            p_val = results['p_values'][var]\n",
    "            \n",
    "            if var == 'const':\n",
    "                expected = 'N/A'\n",
    "                sign_ok = 'N/A'\n",
    "            else:\n",
    "                expected = expected_signs.get(var, '?')\n",
    "                actual_sign = '+' if coef > 0 else '-'\n",
    "                sign_ok = '‚úì' if actual_sign == expected else '‚úó'\n",
    "            \n",
    "            print(f\"{var:<8} {coef:>9.4f} {se:>9.4f} {t_stat:>7.2f} {p_val:>7.4f} {expected:>7s} {sign_ok:>7s}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_regression_table(results, title=\"Regression Results\", save_path=None):\n",
    "    \"\"\"\n",
    "    Create a formatted regression results table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information\n",
    "    n_obs = results['n_obs']\n",
    "    r2 = results['r_squared']\n",
    "    adj_r2 = results['adj_r_squared'] \n",
    "    f_stat = results['f_statistic']\n",
    "    f_pval = results['f_pvalue']\n",
    "    dw = results['durbin_watson']\n",
    "    \n",
    "    # Build table\n",
    "    table_lines = []\n",
    "    table_lines.append(f\"{title}\")\n",
    "    table_lines.append(\"=\" * 70)\n",
    "    table_lines.append(f\"{'Variable':<15} {'Coefficient':<12} {'Std Error':<12} {'t-statistic':<12} {'p-value':<10}\")\n",
    "    table_lines.append(\"-\" * 70)\n",
    "    \n",
    "    # Add coefficients\n",
    "    vars_order = ['const', 'DY', 'PI12', 'DI12', 'DIP12', 'TERM']\n",
    "    for var in vars_order:\n",
    "        if var in results['coefficients']:\n",
    "            coef = results['coefficients'][var]\n",
    "            se = results['std_errors'][var]\n",
    "            t_stat = results['t_stats'][var] \n",
    "            p_val = results['p_values'][var]\n",
    "            \n",
    "            var_label = 'Constant' if var == 'const' else var\n",
    "            table_lines.append(f\"{var_label:<15} {coef:>11.4f} {se:>11.4f} {t_stat:>11.2f} {p_val:>9.4f}\")\n",
    "    \n",
    "    table_lines.append(\"-\" * 70)\n",
    "    table_lines.append(f\"{'Observations':<15} {n_obs:>11d}\")\n",
    "    table_lines.append(f\"{'R¬≤':<15} {r2:>11.4f}\")\n",
    "    table_lines.append(f\"{'Adjusted R¬≤':<15} {adj_r2:>11.4f}\")\n",
    "    table_lines.append(f\"{'F-statistic':<15} {f_stat:>11.2f}\")\n",
    "    table_lines.append(f\"{'Prob(F-stat)':<15} {f_pval:>11.4f}\")\n",
    "    table_lines.append(f\"{'Durbin-Watson':<15} {dw:>11.3f}\")\n",
    "    table_lines.append(\"=\" * 70)\n",
    "    \n",
    "    table_text = \"\\n\".join(table_lines)\n",
    "    print(table_text)\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(table_text)\n",
    "    \n",
    "    return table_text\n",
    "\n",
    "\n",
    "# Run full sample regressions\n",
    "print(\"üöÄ RUNNING FULL SAMPLE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare clean dataset for regression (remove missing values)\n",
    "clean_data = model_data.dropna(subset=['excess_return_6m', 'DY', 'PI12', 'DI12', 'DIP12'])\n",
    "\n",
    "print(f\"üìä Clean regression dataset:\")\n",
    "print(f\"   Original observations: {len(model_data)}\")\n",
    "print(f\"   Clean observations: {len(clean_data)}\")\n",
    "print(f\"   Data coverage: {clean_data.index.min().strftime('%Y-%m')} to {clean_data.index.max().strftime('%Y-%m')}\")\n",
    "\n",
    "# Run primary regression (4 predictors)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ESTIMATING PRIMARY MODEL (4 PREDICTORS)\")\n",
    "print(\"=\"*70)\n",
    "primary_results = run_full_sample_regression(\n",
    "    clean_data, \n",
    "    predictors=['DY', 'PI12', 'DI12', 'DIP12'],\n",
    "    include_term=False\n",
    ")\n",
    "\n",
    "# Create and save regression table\n",
    "table_primary = create_regression_table(\n",
    "    primary_results, \n",
    "    title=\"Table 1: OLS Regression Results - Primary Model (4 Predictors)\",\n",
    "    save_path=OUTPUT_PATH / \"tables\" / \"regression_primary.txt\"\n",
    ")\n",
    "\n",
    "# Run extended regression (5 predictors) if term spread available\n",
    "if 'TERM' in clean_data.columns and clean_data['TERM'].notna().sum() > 50:\n",
    "    print(\"\\n\" + \"=\"*70) \n",
    "    print(\"ESTIMATING EXTENDED MODEL (5 PREDICTORS)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    extended_results = run_full_sample_regression(\n",
    "        clean_data,\n",
    "        predictors=['DY', 'PI12', 'DI12', 'DIP12'],\n",
    "        include_term=True\n",
    "    )\n",
    "    \n",
    "    table_extended = create_regression_table(\n",
    "        extended_results,\n",
    "        title=\"Table 2: OLS Regression Results - Extended Model (5 Predictors)\", \n",
    "        save_path=OUTPUT_PATH / \"tables\" / \"regression_extended.txt\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Term spread data insufficient for extended model\")\n",
    "    extended_results = None\n",
    "\n",
    "print(f\"\\n‚úÖ Full sample regression analysis completed!\")\n",
    "print(f\"üìÅ Results saved to: {OUTPUT_PATH / 'tables'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2dc5a9",
   "metadata": {},
   "source": [
    "## 6. Implement Recursive Forecasting Procedure\n",
    "\n",
    "The core of the Pesaran-Timmermann methodology is **recursive out-of-sample forecasting** using an expanding window:\n",
    "\n",
    "**Procedure for each decision date t:**\n",
    "1. **COLLECT** training data from Jan 1990 to t\n",
    "2. **ESTIMATE** regression: `ERFTSE = Œ± + Œ≤‚ÇÅ√óDY + Œ≤‚ÇÇ√óPI12 + Œ≤‚ÇÉ√óDI12 + Œ≤‚ÇÑ√óDIP12 + Œµ`\n",
    "3. **OBTAIN** predictor values at time t (with appropriate lags)\n",
    "4. **PREDICT** excess return for next 6 months: `œÅÃÇ_{t+6} = Œ±ÃÇ + Œ≤ÃÇ‚ÇÅ√óDY_{t-1} + Œ≤ÃÇ‚ÇÇ√óPI12_{t-2} + Œ≤ÃÇ‚ÇÉ√óDI12_{t-1} + Œ≤ÃÇ‚ÇÑ√óDIP12_{t-2}`\n",
    "5. **DECIDE** trading position: Stocks if œÅÃÇ_{t+6} > 0, Bonds otherwise\n",
    "6. **RECORD** prediction, actual outcome, and trading decision\n",
    "7. **EXPAND** training window and repeat\n",
    "\n",
    "**Timeline:**\n",
    "- **Training starts:** January 1990\n",
    "- **First prediction:** November 2015 - April 2016  \n",
    "- **Last prediction:** May 2025 - October 2025\n",
    "- **Total predictions:** 20 periods (6-month intervals)\n",
    "\n",
    "**No Look-Ahead Bias:** Each prediction uses only information available at the decision date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe917a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast_dates(start_date, end_date, horizon=6):\n",
    "    \"\"\"\n",
    "    Generate decision dates for recursive forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start of out-of-sample period\n",
    "    end_date : str  \n",
    "        End of evaluation period\n",
    "    horizon : int\n",
    "        Forecast horizon in months\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples\n",
    "        (decision_date, forecast_start, forecast_end)\n",
    "    \"\"\"\n",
    "    \n",
    "    decision_dates = []\n",
    "    current_date = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    \n",
    "    while current_date <= end_dt:\n",
    "        # Calculate forecast period \n",
    "        forecast_start = current_date + pd.DateOffset(months=1)\n",
    "        forecast_end = forecast_start + pd.DateOffset(months=horizon-1)\n",
    "        \n",
    "        # Only include if forecast_end is within our data range\n",
    "        if forecast_end <= end_dt:\n",
    "            decision_dates.append((current_date, forecast_start, forecast_end))\n",
    "        \n",
    "        # Move to next decision date (6 months later)\n",
    "        current_date += pd.DateOffset(months=horizon)\n",
    "    \n",
    "    return decision_dates\n",
    "\n",
    "\n",
    "def run_recursive_forecasting(df, predictors=['DY', 'PI12', 'DI12', 'DIP12'],\n",
    "                            target='excess_return_6m', \n",
    "                            training_start='1990-01-31',\n",
    "                            forecast_start='2015-10-31',\n",
    "                            forecast_end='2025-10-31',\n",
    "                            horizon=6):\n",
    "    \"\"\"\n",
    "    Execute recursive out-of-sample forecasting procedure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Model dataset with all variables\n",
    "    predictors : list\n",
    "        List of predictor variables  \n",
    "    target : str\n",
    "        Target variable name\n",
    "    training_start : str\n",
    "        Start of training period\n",
    "    forecast_start : str\n",
    "        Start of out-of-sample forecasting\n",
    "    forecast_end : str\n",
    "        End of evaluation period\n",
    "    horizon : int\n",
    "        Forecast horizon in months\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Results with predictions and actuals\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîÆ RECURSIVE OUT-OF-SAMPLE FORECASTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Generate forecast dates\n",
    "    forecast_dates = generate_forecast_dates(forecast_start, forecast_end, horizon)\n",
    "    \n",
    "    print(f\"üìÖ Forecast schedule:\")\n",
    "    print(f\"   Training start: {training_start}\")\n",
    "    print(f\"   First forecast: {forecast_dates[0][1].strftime('%Y-%m')} to {forecast_dates[0][2].strftime('%Y-%m')}\")  \n",
    "    print(f\"   Last forecast: {forecast_dates[-1][1].strftime('%Y-%m')} to {forecast_dates[-1][2].strftime('%Y-%m')}\")\n",
    "    print(f\"   Total forecasts: {len(forecast_dates)}\\\")\\n\")\n",
    "    \n",
    "    results = []\\n    training_start_dt = pd.to_datetime(training_start)\n",
    "    \n",
    "    for i, (decision_date, forecast_start_date, forecast_end_date) in enumerate(forecast_dates):\n",
    "        \n",
    "        print(f\"üéØ Forecast {i+1:2d}/{len(forecast_dates)}: Decision {decision_date.strftime('%Y-%m-%d')} ‚Üí Forecast {forecast_start_date.strftime('%Y-%m')} to {forecast_end_date.strftime('%Y-%m')}\")\n",
    "        \n",
    "        # 1. Collect training data up to decision date\n",
    "        training_mask = (df.index >= training_start_dt) & (df.index <= decision_date)\n",
    "        training_data = df[training_mask].copy()\n",
    "        \n",
    "        # Remove rows with missing values in regression variables\n",
    "        reg_vars = [target] + predictors\n",
    "        training_clean = training_data[reg_vars].dropna()\n",
    "        \n",
    "        if len(training_clean) < 20:  # Minimum sample size\n",
    "            print(f\"   ‚ö†Ô∏è  Insufficient training data: {len(training_clean)} obs\")\n",
    "            continue\n",
    "        \n",
    "        # 2. Estimate regression model\n",
    "        y_train = training_clean[target]\n",
    "        X_train = training_clean[predictors] \n",
    "        X_train_const = sm.add_constant(X_train)\n",
    "        \n",
    "        try:\n",
    "            model = sm.OLS(y_train, X_train_const).fit()\n",
    "        except:\n",
    "            print(f\"   ‚ùå Regression estimation failed\")\n",
    "            continue\n",
    "        \n",
    "        # 3. Get predictor values at decision date (with appropriate lags)\n",
    "        # Predictors are already lagged in construction, so use decision_date values\n",
    "        try:\n",
    "            predictor_values = df.loc[decision_date, predictors]\n",
    "            \n",
    "            # Check for missing predictor values\n",
    "            if predictor_values.isna().any():\n",
    "                print(f\"   ‚ö†Ô∏è  Missing predictor values at decision date\")\n",
    "                print(f\"      Missing: {predictor_values[predictor_values.isna()].index.tolist()}\")\n",
    "                # Forward fill from recent values\n",
    "                for var in predictors:\n",
    "                    if pd.isna(predictor_values[var]):\n",
    "                        recent_values = df.loc[:decision_date, var].dropna()\n",
    "                        if len(recent_values) > 0:\n",
    "                            predictor_values[var] = recent_values.iloc[-1]\n",
    "                            print(f\"      {var}: filled with {predictor_values[var]:.4f}\")\n",
    "        \n",
    "        except KeyError:\n",
    "            print(f\"   ‚ùå Decision date {decision_date} not in dataset\")\n",
    "            continue\n",
    "        \n",
    "        # 4. Make prediction\n",
    "        X_forecast = np.concatenate([[1], predictor_values.values])  # Add constant\n",
    "        predicted_excess_return = np.dot(model.params, X_forecast)\n",
    "        \n",
    "        # 5. Get actual outcome (if available)\n",
    "        actual_excess_return = np.nan\n",
    "        actual_stock_return = np.nan\n",
    "        actual_bond_return = np.nan\n",
    "        \n",
    "        # Find actual return over forecast period\n",
    "        forecast_dates_in_data = df.index[(df.index >= forecast_start_date) & (df.index <= forecast_end_date)]\n",
    "        if len(forecast_dates_in_data) > 0:\n",
    "            # Use the last available observation in the forecast period\n",
    "            actual_date = forecast_dates_in_data[-1]\n",
    "            try:\n",
    "                actual_excess_return = df.loc[actual_date, target]\n",
    "                # Calculate components for trading strategy\n",
    "                actual_stock_return = df.loc[actual_date, 'ftse_return_6m'] \n",
    "                actual_bond_return = df.loc[actual_date, 'rf_6m']\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        # 6. Make trading decision\n",
    "        trading_decision = 'STOCKS' if predicted_excess_return > 0 else 'BONDS'\n",
    "        \n",
    "        # 7. Record results\n",
    "        result = {\n",
    "            'decision_date': decision_date,\n",
    "            'forecast_start': forecast_start_date,\n",
    "            'forecast_end': forecast_end_date,\n",
    "            'training_observations': len(training_clean),\n",
    "            'predicted_excess_return': predicted_excess_return,\n",
    "            'actual_excess_return': actual_excess_return,\n",
    "            'actual_stock_return': actual_stock_return,\n",
    "            'actual_bond_return': actual_bond_return,\n",
    "            'trading_decision': trading_decision,\n",
    "            'model_r_squared': model.rsquared,\n",
    "            'predictor_values': dict(predictor_values)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Display prediction\n",
    "        print(f\"      Training obs: {len(training_clean):3d}, R¬≤ = {model.rsquared:.3f}\")\n",
    "        print(f\"      Predicted ER: {predicted_excess_return:7.4f} ‚Üí {trading_decision}\")\n",
    "        if not pd.isna(actual_excess_return):\n",
    "            sign_correct = (predicted_excess_return > 0) == (actual_excess_return > 0)\n",
    "            print(f\"      Actual ER:    {actual_excess_return:7.4f} ‚Üí Sign correct: {sign_correct}\")\n",
    "        print()\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"‚úÖ Recursive forecasting completed!\")\n",
    "    print(f\"   Generated {len(results_df)} predictions\")\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    if len(results_df) > 0:\n",
    "        valid_actuals = results_df['actual_excess_return'].dropna()\n",
    "        valid_predictions = results_df.loc[valid_actuals.index, 'predicted_excess_return']\n",
    "        \n",
    "        if len(valid_actuals) > 0:\n",
    "            # Sign accuracy\n",
    "            sign_accuracy = ((valid_predictions > 0) == (valid_actuals > 0)).mean()\n",
    "            \n",
    "            print(f\"   Sign accuracy: {sign_accuracy:.1%} ({(valid_predictions > 0).sum()}/{len(valid_predictions)})\")\n",
    "            print(f\"   Mean prediction: {valid_predictions.mean():.4f}\")\n",
    "            print(f\"   Mean actual: {valid_actuals.mean():.4f}\")\n",
    "            print(f\"   Correlation: {valid_predictions.corr(valid_actuals):.3f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Execute recursive forecasting\n",
    "print(\"üöÄ STARTING RECURSIVE FORECASTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run recursive forecasting with primary model\n",
    "recursive_results = run_recursive_forecasting(\n",
    "    df=clean_data,\n",
    "    predictors=['DY', 'PI12', 'DI12', 'DIP12'],\n",
    "    target='excess_return_6m',\n",
    "    training_start=TRAINING_START,\n",
    "    forecast_start=FORECAST_START, \n",
    "    forecast_end=FORECAST_END,\n",
    "    horizon=FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "# Display results summary\n",
    "print(f\"\\nüìä RECURSIVE FORECASTING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total predictions generated: {len(recursive_results)}\")\n",
    "print(f\"Period: {recursive_results['forecast_start'].min().strftime('%Y-%m')} to {recursive_results['forecast_end'].max().strftime('%Y-%m')}\")\n",
    "\n",
    "# Save results\n",
    "recursive_results.to_csv(OUTPUT_PATH / \"recursive_forecasting_results.csv\", index=False)\n",
    "recursive_results.to_excel(OUTPUT_PATH / \"recursive_forecasting_results.xlsx\", index=False)\n",
    "\n",
    "print(f\"üìÅ Results saved to: {OUTPUT_PATH}\")\n",
    "print(\"‚úÖ Recursive forecasting completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ba219",
   "metadata": {},
   "source": [
    "## 7. Execute Pesaran-Timmermann Sign Test\n",
    "\n",
    "The **Pesaran-Timmermann (PT) test** evaluates whether the model predicts the **direction** of excess returns better than random chance. \n",
    "\n",
    "**Key Features:**\n",
    "- **Distribution-free** (works with non-normal returns)\n",
    "- **Focuses on direction** (what matters for trading)  \n",
    "- **Statistically rigorous** (formal hypothesis test)\n",
    "\n",
    "**Test Procedure:**\n",
    "1. Count correct sign predictions: `PÃÇ = n_correct / n`\n",
    "2. Calculate expected accuracy under independence: `P* = P_y √ó P_≈∑ + (1-P_y) √ó (1-P_≈∑)`\n",
    "3. Compute test statistic: `PT = (PÃÇ - P*) / SE`\n",
    "4. Compare to critical values: 1.645 (5%), 2.326 (1%)\n",
    "\n",
    "**Economic Interpretation:**\n",
    "- **H‚ÇÄ:** Signs are independent (no forecasting ability)\n",
    "- **H‚ÇÅ:** Model has directional forecasting ability  \n",
    "- **Rejection:** Evidence of genuine forecasting skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c42b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesaran_timmermann_test(actual, predicted):\n",
    "    \"\"\"\n",
    "    Compute the Pesaran-Timmermann directional accuracy test.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    actual : array-like\n",
    "        Actual excess returns\n",
    "    predicted : array-like\n",
    "        Predicted excess returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Test results including statistic, p-value, and accuracy metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    n = len(actual)\n",
    "    \n",
    "    if n == 0:\n",
    "        return {'error': 'No valid observations'}\n",
    "    \n",
    "    # Convert to signs (1 = positive, 0 = negative/zero)\n",
    "    sign_actual = (actual > 0).astype(int)\n",
    "    sign_predicted = (predicted > 0).astype(int)\n",
    "    \n",
    "    # Proportion of correct sign predictions\n",
    "    correct = (sign_actual == sign_predicted)\n",
    "    P_hat = correct.mean()\n",
    "    \n",
    "    # Proportions of positive returns\n",
    "    P_y = sign_actual.mean()        # Proportion of positive actual returns\n",
    "    P_y_hat = sign_predicted.mean() # Proportion of positive predicted returns\n",
    "    \n",
    "    # Expected proportion correct under null hypothesis (independence)\n",
    "    P_star = P_y * P_y_hat + (1 - P_y) * (1 - P_y_hat)\n",
    "    \n",
    "    # Variance calculations\n",
    "    # Variance of P_hat under null\n",
    "    Var_P_hat = P_star * (1 - P_star) / n\n",
    "    \n",
    "    # Variance of P_star  \n",
    "    Var_P_star = ((2 * P_y - 1)**2 * P_y_hat * (1 - P_y_hat) / n +\n",
    "                  (2 * P_y_hat - 1)**2 * P_y * (1 - P_y) / n +\n",
    "                  4 * P_y * P_y_hat * (1 - P_y) * (1 - P_y_hat) / n**2)\n",
    "    \n",
    "    # Standard error\n",
    "    SE = np.sqrt(Var_P_hat - Var_P_star) if (Var_P_hat - Var_P_star) > 0 else 0\n",
    "    \n",
    "    # Test statistic  \n",
    "    if SE > 0:\n",
    "        PT_statistic = (P_hat - P_star) / SE\n",
    "        p_value = 1 - stats.norm.cdf(PT_statistic)  # One-sided test\n",
    "    else:\n",
    "        PT_statistic = np.nan\n",
    "        p_value = np.nan\n",
    "    \n",
    "    return {\n",
    "        'n_predictions': n,\n",
    "        'n_correct': int(correct.sum()),\n",
    "        'accuracy': P_hat,\n",
    "        'expected_accuracy': P_star,\n",
    "        'prop_positive_actual': P_y,\n",
    "        'prop_positive_predicted': P_y_hat,\n",
    "        'PT_statistic': PT_statistic,\n",
    "        'p_value': p_value,\n",
    "        'standard_error': SE,\n",
    "        'variance_P_hat': Var_P_hat,\n",
    "        'variance_P_star': Var_P_star,\n",
    "        'significant_5pct': PT_statistic >= 1.645 if not np.isnan(PT_statistic) else False,\n",
    "        'significant_1pct': PT_statistic >= 2.326 if not np.isnan(PT_statistic) else False\n",
    "    }\n",
    "\n",
    "\n",
    "def create_sign_test_table(pt_results, title=\"Pesaran-Timmermann Sign Test Results\"):\n",
    "    \"\"\"\n",
    "    Create formatted table for PT test results.\n",
    "    \"\"\"\n",
    "    \n",
    "    table_lines = []\n",
    "    table_lines.append(title)\n",
    "    table_lines.append(\"=\" * 65)\n",
    "    table_lines.append(f\"{'Metric':<35} {'Value':<20} {'Notes':<10}\")\n",
    "    table_lines.append(\"-\" * 65)\n",
    "    \n",
    "    # Basic statistics\n",
    "    table_lines.append(f\"{'Number of predictions':<35} {pt_results['n_predictions']:<20d}\")\n",
    "    table_lines.append(f\"{'Correct sign predictions':<35} {pt_results['n_correct']:<20d}\")\n",
    "    table_lines.append(f\"{'Proportion correct (%)':<35} {pt_results['accuracy']*100:<20.1f}\")\n",
    "    table_lines.append(f\"{'Expected proportion under H0 (%)':<35} {pt_results['expected_accuracy']*100:<20.1f}\")\n",
    "    \n",
    "    table_lines.append(\"-\" * 65)\n",
    "    \n",
    "    # Test statistics\n",
    "    table_lines.append(f\"{'PT test statistic':<35} {pt_results['PT_statistic']:<20.3f}\")\n",
    "    table_lines.append(f\"{'p-value (one-sided)':<35} {pt_results['p_value']:<20.4f}\")\n",
    "    table_lines.append(f\"{'Standard error':<35} {pt_results['standard_error']:<20.4f}\")\n",
    "    \n",
    "    table_lines.append(\"-\" * 65)\n",
    "    \n",
    "    # Significance\n",
    "    sig_5 = \"Yes\" if pt_results['significant_5pct'] else \"No\"\n",
    "    sig_1 = \"Yes\" if pt_results['significant_1pct'] else \"No\" \n",
    "    table_lines.append(f\"{'Significant at 5%?':<35} {sig_5:<20}\")\n",
    "    table_lines.append(f\"{'Significant at 1%?':<35} {sig_1:<20}\")\n",
    "    \n",
    "    table_lines.append(\"-\" * 65)\n",
    "    \n",
    "    # Additional metrics\n",
    "    table_lines.append(f\"{'Prop. positive actual (%)':<35} {pt_results['prop_positive_actual']*100:<20.1f}\")\n",
    "    table_lines.append(f\"{'Prop. positive predicted (%)':<35} {pt_results['prop_positive_predicted']*100:<20.1f}\")\n",
    "    \n",
    "    table_lines.append(\"=\" * 65)\n",
    "    \n",
    "    # Add interpretation\n",
    "    table_lines.append(\"\")\n",
    "    table_lines.append(\"INTERPRETATION:\")\n",
    "    if pt_results['significant_5pct']:\n",
    "        table_lines.append(f\"‚úÖ Model demonstrates significant directional forecasting ability\")\n",
    "        table_lines.append(f\"   (PT statistic {pt_results['PT_statistic']:.3f} > 1.645 critical value)\")\n",
    "    else:\n",
    "        table_lines.append(f\"‚ùå No significant evidence of directional forecasting ability\") \n",
    "        table_lines.append(f\"   (PT statistic {pt_results['PT_statistic']:.3f} < 1.645 critical value)\")\n",
    "    \n",
    "    if pt_results['accuracy'] > 0.6:\n",
    "        table_lines.append(f\"üìà High sign accuracy: {pt_results['accuracy']*100:.1f}% correct predictions\")\n",
    "    elif pt_results['accuracy'] > 0.55:\n",
    "        table_lines.append(f\"üìä Moderate sign accuracy: {pt_results['accuracy']*100:.1f}% correct predictions\")\n",
    "    else:\n",
    "        table_lines.append(f\"üìâ Low sign accuracy: {pt_results['accuracy']*100:.1f}% correct predictions\")\n",
    "    \n",
    "    table_text = \"\\n\".join(table_lines)\n",
    "    return table_text\n",
    "\n",
    "\n",
    "# Execute Pesaran-Timmermann test\n",
    "print(\"üß™ PESARAN-TIMMERMANN SIGN TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract predictions and actuals (only valid pairs)\n",
    "valid_mask = recursive_results['actual_excess_return'].notna()\n",
    "valid_results = recursive_results[valid_mask].copy()\n",
    "\n",
    "if len(valid_results) == 0:\n",
    "    print(\"‚ùå No valid prediction-actual pairs available for testing\")\n",
    "else:\n",
    "    actual_returns = valid_results['actual_excess_return'].values\n",
    "    predicted_returns = valid_results['predicted_excess_return'].values\n",
    "    \n",
    "    print(f\"üìä Test data summary:\")\n",
    "    print(f\"   Valid predictions: {len(actual_returns)}\")\n",
    "    print(f\"   Actual returns - positive: {(actual_returns > 0).sum()}/{len(actual_returns)} ({(actual_returns > 0).mean()*100:.1f}%)\")\n",
    "    print(f\"   Predicted returns - positive: {(predicted_returns > 0).sum()}/{len(predicted_returns)} ({(predicted_returns > 0).mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Run PT test\n",
    "    pt_results = pesaran_timmermann_test(actual_returns, predicted_returns)\n",
    "    \n",
    "    if 'error' in pt_results:\n",
    "        print(f\"‚ùå Test failed: {pt_results['error']}\")\n",
    "    else:\n",
    "        print(f\"\\nüìã SIGN TEST RESULTS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Create and display results table\n",
    "        sign_test_table = create_sign_test_table(pt_results)\n",
    "        print(sign_test_table)\n",
    "        \n",
    "        # Save results\n",
    "        with open(OUTPUT_PATH / \"tables\" / \"pesaran_timmermann_test.txt\", 'w') as f:\n",
    "            f.write(sign_test_table)\n",
    "        \n",
    "        # Additional analysis\n",
    "        print(f\"\\nüìà DETAILED SIGN ANALYSIS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Create contingency table\n",
    "        sign_actual = (actual_returns > 0).astype(int)\n",
    "        sign_predicted = (predicted_returns > 0).astype(int)\n",
    "        \n",
    "        # 2x2 contingency table\n",
    "        correct_pos = ((sign_actual == 1) & (sign_predicted == 1)).sum()  # True positives\n",
    "        correct_neg = ((sign_actual == 0) & (sign_predicted == 0)).sum()  # True negatives  \n",
    "        wrong_pos = ((sign_actual == 0) & (sign_predicted == 1)).sum()    # False positives\n",
    "        wrong_neg = ((sign_actual == 1) & (sign_predicted == 0)).sum()    # False negatives\n",
    "        \n",
    "        print(f\"Contingency Table:\")\n",
    "        print(f\"                    Predicted\")\n",
    "        print(f\"Actual         Negative  Positive  Total\")\n",
    "        print(f\"Negative       {correct_neg:8d}  {wrong_pos:8d}  {correct_neg + wrong_pos:5d}\")\n",
    "        print(f\"Positive       {wrong_neg:8d}  {correct_pos:8d}  {wrong_neg + correct_pos:5d}\")\n",
    "        print(f\"Total          {correct_neg + wrong_neg:8d}  {wrong_pos + correct_pos:8d}  {len(actual_returns):5d}\")\n",
    "        \n",
    "        print(f\"\\nPrediction Quality:\")\n",
    "        if len(actual_returns) > 0:\n",
    "            sensitivity = correct_pos / (correct_pos + wrong_neg) if (correct_pos + wrong_neg) > 0 else 0\n",
    "            specificity = correct_neg / (correct_neg + wrong_pos) if (correct_neg + wrong_pos) > 0 else 0\n",
    "            print(f\"   Sensitivity (correct positive): {sensitivity:.1%}\")\n",
    "            print(f\"   Specificity (correct negative): {specificity:.1%}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Pesaran-Timmermann test completed!\")\n",
    "        print(f\"üìÅ Results saved to: {OUTPUT_PATH / 'tables' / 'pesaran_timmermann_test.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92298666",
   "metadata": {},
   "source": [
    "## 8. Implement Trading Strategy with Transaction Costs\n",
    "\n",
    "Implement the **switching strategy** based on model predictions:\n",
    "\n",
    "**Trading Rule:**\n",
    "- **IF** predicted_excess_return > 0: **HOLD STOCKS** (FTSE All-Share)\n",
    "- **ELSE**: **HOLD BONDS** (UK T-bills)\n",
    "\n",
    "**Transaction Cost Scenarios:**\n",
    "- **Zero (0.0%):** Benchmark for pure model performance  \n",
    "- **Low (0.25%):** Modern ETF/index fund trading\n",
    "- **Medium (0.5%):** Realistic for individual investors\n",
    "- **High (1.0%):** Conservative estimate/historical costs\n",
    "\n",
    "**Cost Structure:**\n",
    "- Applied only when **switching positions** (Stocks ‚Üî Bonds)\n",
    "- No costs for **holding** the same position\n",
    "- Bonds assumed to be held to maturity (no transaction costs)\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Mean 6-month returns\n",
    "- Standard deviation (risk measure) \n",
    "- Sharpe ratio (risk-adjusted performance)\n",
    "- Total cumulative return\n",
    "- Maximum drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d718f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_switching_strategy_returns(results_df, transaction_costs=[0.0, 0.0025, 0.005, 0.01]):\n",
    "    \"\"\"\n",
    "    Calculate returns for switching strategy under different transaction cost scenarios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        Recursive forecasting results\n",
    "    transaction_costs : list\n",
    "        List of transaction cost rates to test\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Performance results by transaction cost scenario\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üíº TRADING STRATEGY IMPLEMENTATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Filter to valid observations only\n",
    "    valid_data = results_df[\n",
    "        results_df['actual_excess_return'].notna() & \n",
    "        results_df['actual_stock_return'].notna() & \n",
    "        results_df['actual_bond_return'].notna()\n",
    "    ].copy()\n",
    "    \n",
    "    if len(valid_data) == 0:\n",
    "        print(\"‚ùå No valid trading periods available\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"üìä Trading analysis:\")\n",
    "    print(f\"   Valid trading periods: {len(valid_data)}\")\n",
    "    print(f\"   Period: {valid_data['forecast_start'].min().strftime('%Y-%m')} to {valid_data['forecast_end'].max().strftime('%Y-%m')}\")\n",
    "    \n",
    "    # Extract data\n",
    "    predictions = valid_data['predicted_excess_return'].values\n",
    "    stock_returns = valid_data['actual_stock_return'].values\n",
    "    bond_returns = valid_data['actual_bond_return'].values\n",
    "    trading_decisions = valid_data['trading_decision'].values\n",
    "    \n",
    "    # Count trading decisions\n",
    "    stocks_periods = (trading_decisions == 'STOCKS').sum()\n",
    "    bonds_periods = (trading_decisions == 'BONDS').sum()\n",
    "    \n",
    "    print(f\"   Stock positions: {stocks_periods} ({stocks_periods/len(valid_data)*100:.1f}%)\")\n",
    "    print(f\"   Bond positions: {bonds_periods} ({bonds_periods/len(valid_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate number of switches\n",
    "    switches = 0\n",
    "    for i in range(1, len(trading_decisions)):\n",
    "        if trading_decisions[i] != trading_decisions[i-1]:\n",
    "            switches += 1\n",
    "    \n",
    "    print(f\"   Total switches: {switches}\")\n",
    "    \n",
    "    # Performance analysis for each transaction cost scenario\n",
    "    cost_names = ['Zero', 'Low (0.25%)', 'Medium (0.5%)', 'High (1.0%)']\n",
    "    performance_results = {}\n",
    "    \n",
    "    print(f\"\\nüìà STRATEGY PERFORMANCE BY TRANSACTION COST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, cost in enumerate(transaction_costs):\n",
    "        cost_name = cost_names[i] if i < len(cost_names) else f\"{cost*100:.2f}%\"\n",
    "        \n",
    "        # Calculate portfolio returns\n",
    "        portfolio_returns = []\n",
    "        previous_position = None\n",
    "        \n",
    "        for j in range(len(predictions)):\n",
    "            # Determine position based on prediction\n",
    "            current_position = 'STOCKS' if predictions[j] > 0 else 'BONDS'\n",
    "            \n",
    "            # Get gross return\n",
    "            if current_position == 'STOCKS':\n",
    "                gross_return = stock_returns[j]\n",
    "            else:\n",
    "                gross_return = bond_returns[j]\n",
    "            \n",
    "            # Apply transaction cost if switching\n",
    "            if previous_position is not None and current_position != previous_position:\n",
    "                net_return = gross_return - cost\n",
    "            else:\n",
    "                net_return = gross_return\n",
    "            \n",
    "            portfolio_returns.append(net_return)\n",
    "            previous_position = current_position\n",
    "        \n",
    "        portfolio_returns = np.array(portfolio_returns)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        mean_return = np.mean(portfolio_returns)\n",
    "        std_return = np.std(portfolio_returns, ddof=1) if len(portfolio_returns) > 1 else 0\n",
    "        total_return = np.prod(1 + portfolio_returns) - 1\n",
    "        \n",
    "        # Annualized metrics (6-month periods ‚Üí multiply by 2 for annual)\n",
    "        annual_mean = mean_return * 2\n",
    "        annual_std = std_return * np.sqrt(2)\n",
    "        sharpe_ratio = annual_mean / annual_std if annual_std > 0 else 0\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        cumulative_returns = np.cumprod(1 + portfolio_returns)\n",
    "        running_max = np.maximum.accumulate(cumulative_returns)\n",
    "        drawdown = (cumulative_returns - running_max) / running_max\n",
    "        max_drawdown = np.min(drawdown)\n",
    "        \n",
    "        # Store results\n",
    "        performance_results[cost_name] = {\n",
    "            'transaction_cost': cost,\n",
    "            'mean_return': mean_return,\n",
    "            'std_return': std_return,\n",
    "            'annual_mean': annual_mean,\n",
    "            'annual_std': annual_std,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'total_return': total_return,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'portfolio_returns': portfolio_returns,\n",
    "            'n_switches': switches\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{cost_name} Transaction Costs ({cost*100:.2f}%):\")\n",
    "        print(f\"   Mean 6-month return: {mean_return*100:6.2f}% (Annual: {annual_mean*100:6.2f}%)\")\n",
    "        print(f\"   Volatility:          {std_return*100:6.2f}% (Annual: {annual_std*100:6.2f}%)\")\n",
    "        print(f\"   Sharpe ratio:        {sharpe_ratio:6.3f}\")\n",
    "        print(f\"   Total return:        {total_return*100:6.2f}%\")\n",
    "        print(f\"   Max drawdown:        {max_drawdown*100:6.2f}%\")\n",
    "    \n",
    "    return performance_results\n",
    "\n",
    "\n",
    "def calculate_benchmark_returns(results_df):\n",
    "    \"\"\"\n",
    "    Calculate benchmark strategy returns for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä BENCHMARK STRATEGIES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_data = results_df[\n",
    "        results_df['actual_stock_return'].notna() & \n",
    "        results_df['actual_bond_return'].notna()\n",
    "    ].copy()\n",
    "    \n",
    "    if len(valid_data) == 0:\n",
    "        return {}\n",
    "    \n",
    "    stock_returns = valid_data['actual_stock_return'].values\n",
    "    bond_returns = valid_data['actual_bond_return'].values\n",
    "    \n",
    "    benchmarks = {}\n",
    "    \n",
    "    # 1. Buy and Hold Stocks (FTSE All-Share)\n",
    "    bh_mean = np.mean(stock_returns)\n",
    "    bh_std = np.std(stock_returns, ddof=1) if len(stock_returns) > 1 else 0\n",
    "    bh_total = np.prod(1 + stock_returns) - 1\n",
    "    bh_annual_mean = bh_mean * 2\n",
    "    bh_annual_std = bh_std * np.sqrt(2)\n",
    "    bh_sharpe = bh_annual_mean / bh_annual_std if bh_annual_std > 0 else 0\n",
    "    \n",
    "    # Max drawdown for buy-and-hold\n",
    "    bh_cumulative = np.cumprod(1 + stock_returns)\n",
    "    bh_running_max = np.maximum.accumulate(bh_cumulative)\n",
    "    bh_drawdown = (bh_cumulative - bh_running_max) / bh_running_max\n",
    "    bh_max_drawdown = np.min(bh_drawdown)\n",
    "    \n",
    "    benchmarks['Buy-and-Hold FTSE'] = {\n",
    "        'mean_return': bh_mean,\n",
    "        'std_return': bh_std,\n",
    "        'annual_mean': bh_annual_mean,\n",
    "        'annual_std': bh_annual_std,\n",
    "        'sharpe_ratio': bh_sharpe,\n",
    "        'total_return': bh_total,\n",
    "        'max_drawdown': bh_max_drawdown,\n",
    "        'returns': stock_returns\n",
    "    }\n",
    "    \n",
    "    # 2. Always Bonds (UK T-Bills)\n",
    "    bonds_mean = np.mean(bond_returns)\n",
    "    bonds_std = np.std(bond_returns, ddof=1) if len(bond_returns) > 1 else 0\n",
    "    bonds_total = np.prod(1 + bond_returns) - 1\n",
    "    bonds_annual_mean = bonds_mean * 2\n",
    "    bonds_annual_std = bonds_std * np.sqrt(2)\n",
    "    bonds_sharpe = bonds_annual_mean / bonds_annual_std if bonds_annual_std > 0 else 0\n",
    "    \n",
    "    benchmarks['Always Bonds'] = {\n",
    "        'mean_return': bonds_mean,\n",
    "        'std_return': bonds_std,\n",
    "        'annual_mean': bonds_annual_mean,\n",
    "        'annual_std': bonds_annual_std,\n",
    "        'sharpe_ratio': bonds_sharpe,\n",
    "        'total_return': bonds_total,\n",
    "        'max_drawdown': 0.0,  # Bonds assumed risk-free\n",
    "        'returns': bond_returns\n",
    "    }\n",
    "    \n",
    "    # 3. Random Switching (50/50 each period)\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    random_decisions = np.random.choice(['STOCKS', 'BONDS'], size=len(stock_returns), p=[0.5, 0.5])\n",
    "    random_returns = np.where(random_decisions == 'STOCKS', stock_returns, bond_returns)\n",
    "    \n",
    "    random_mean = np.mean(random_returns)\n",
    "    random_std = np.std(random_returns, ddof=1) if len(random_returns) > 1 else 0\n",
    "    random_total = np.prod(1 + random_returns) - 1\n",
    "    random_annual_mean = random_mean * 2\n",
    "    random_annual_std = random_std * np.sqrt(2)\n",
    "    random_sharpe = random_annual_mean / random_annual_std if random_annual_std > 0 else 0\n",
    "    \n",
    "    benchmarks['Random Switching'] = {\n",
    "        'mean_return': random_mean,\n",
    "        'std_return': random_std,\n",
    "        'annual_mean': random_annual_mean, \n",
    "        'annual_std': random_annual_std,\n",
    "        'sharpe_ratio': random_sharpe,\n",
    "        'total_return': random_total,\n",
    "        'max_drawdown': 0.0,  # Not calculated for random\n",
    "        'returns': random_returns\n",
    "    }\n",
    "    \n",
    "    # Display benchmark results\n",
    "    for name, metrics in benchmarks.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"   Mean 6-month return: {metrics['mean_return']*100:6.2f}% (Annual: {metrics['annual_mean']*100:6.2f}%)\")\n",
    "        print(f\"   Volatility:          {metrics['std_return']*100:6.2f}% (Annual: {metrics['annual_std']*100:6.2f}%)\")\n",
    "        print(f\"   Sharpe ratio:        {metrics['sharpe_ratio']:6.3f}\")\n",
    "        print(f\"   Total return:        {metrics['total_return']*100:6.2f}%\")\n",
    "        if metrics['max_drawdown'] < 0:\n",
    "            print(f\"   Max drawdown:        {metrics['max_drawdown']*100:6.2f}%\")\n",
    "    \n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "# Execute trading strategy analysis\n",
    "trading_performance = calculate_switching_strategy_returns(recursive_results)\n",
    "benchmark_performance = calculate_benchmark_returns(recursive_results)\n",
    "\n",
    "# Combine results for comparison\n",
    "print(f\"\\nüìã PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_strategies = {**trading_performance, **benchmark_performance}\n",
    "\n",
    "if all_strategies:\n",
    "    # Create comparison table\n",
    "    print(f\"{'Strategy':<20} {'Mean Ret':<10} {'Volatility':<10} {'Sharpe':<8} {'Total Ret':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, metrics in all_strategies.items():\n",
    "        mean_ret = metrics['annual_mean'] * 100\n",
    "        vol = metrics['annual_std'] * 100\n",
    "        sharpe = metrics['sharpe_ratio']\n",
    "        total_ret = metrics['total_return'] * 100\n",
    "        \n",
    "        print(f\"{name:<20} {mean_ret:>8.2f}% {vol:>8.2f}% {sharpe:>7.3f} {total_ret:>8.2f}%\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Key insights\n",
    "    if 'Zero' in trading_performance and 'Buy-and-Hold FTSE' in benchmark_performance:\n",
    "        switching_sharpe = trading_performance['Zero']['sharpe_ratio']\n",
    "        bh_sharpe = benchmark_performance['Buy-and-Hold FTSE']['sharpe_ratio']\n",
    "        \n",
    "        print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "        if switching_sharpe > bh_sharpe:\n",
    "            print(f\"   ‚úÖ Switching strategy outperforms buy-and-hold (Sharpe: {switching_sharpe:.3f} vs {bh_sharpe:.3f})\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Switching strategy underperforms buy-and-hold (Sharpe: {switching_sharpe:.3f} vs {bh_sharpe:.3f})\")\n",
    "        \n",
    "        # Check transaction cost impact\n",
    "        if 'High (1.0%)' in trading_performance:\n",
    "            high_cost_sharpe = trading_performance['High (1.0%)']['sharpe_ratio'] \n",
    "            if high_cost_sharpe > bh_sharpe:\n",
    "                print(f\"   üí™ Strategy robust to high transaction costs\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  High transaction costs erode performance\")\n",
    "\n",
    "print(f\"\\n‚úÖ Trading strategy analysis completed!\")\n",
    "print(f\"üìÅ Results can be exported to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853143fe",
   "metadata": {},
   "source": [
    "## 9. Performance Evaluation and Benchmarking\n",
    "\n",
    "Compare the switching strategy against multiple benchmarks to evaluate economic significance:\n",
    "\n",
    "**Comparison Benchmarks:**\n",
    "1. **Buy-and-Hold FTSE:** Always hold stocks (market performance)\n",
    "2. **Always Bonds:** Always hold T-bills (risk-free performance) \n",
    "3. **Random Switching:** Random 50/50 allocation each period\n",
    "4. **Switching Strategy:** Model-based predictions (various transaction costs)\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Mean Return:** Average period performance\n",
    "- **Volatility:** Standard deviation of returns (risk measure)\n",
    "- **Sharpe Ratio:** Risk-adjusted performance `(Mean - RF) / Volatility`\n",
    "- **Total Return:** Cumulative wealth growth over evaluation period\n",
    "- **Maximum Drawdown:** Largest peak-to-trough decline\n",
    "\n",
    "**Success Criteria:**\n",
    "- Switching strategy should outperform buy-and-hold on risk-adjusted basis\n",
    "- Performance should be robust to realistic transaction costs  \n",
    "- Sharpe ratio improvement demonstrates genuine economic value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38038658",
   "metadata": {},
   "source": [
    "## 10. Generate Summary Tables and Visualizations\n",
    "\n",
    "Create comprehensive tables and charts for coursework submission:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
